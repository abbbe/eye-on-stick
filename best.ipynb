{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternate-adapter",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os, urllib, time\n",
    "os.environ[\"MLFLOW_TRACKING_URI\"] = \"sqlite:///mlruns/db.sqlite\"\n",
    "import mlflow, git\n",
    "mlflow_client = mlflow.tracking.MlflowClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "federal-senator",
   "metadata": {},
   "outputs": [],
   "source": [
    "NJ = 4\n",
    "experiment_id = \"0\"\n",
    "run_name = f\"eos.{NJ}J\"\n",
    "\n",
    "rank_metrics = ['|alpha_tmu|', '|eyeerr_tmu|', '-reward_total']\n",
    "print_metrics = ['model_version', 'alpha_tmu', 'alpha_tsigma', 'alpha_tdf', 'eyeerr_tmu', 'eyeerr_tsigma', 'eyeerr_tdf', 'reward_total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "common-seventh",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_runs = mlflow_client.search_runs(experiment_id, f\"tags.mlflow.runName='{run_name}'\")\n",
    "\n",
    "best_metric = dict()\n",
    "best_run = dict()\n",
    "\n",
    "for parent_run in parent_runs:\n",
    "    child_runs = mlflow_client.search_runs(experiment_id, f\"tags.mlflow.parentRunId='{parent_run.info.run_id}'\")\n",
    "    #                                           order_by=[\"metrics.reward_total DESC\"])\n",
    "    for child_run in child_runs:\n",
    "        if 'alpha_tmu' not in child_run.data.metrics:\n",
    "            continue\n",
    "            \n",
    "        child_run.data.metrics['|alpha_tmu|'] = np.abs(child_run.data.metrics['alpha_tmu'])\n",
    "        child_run.data.metrics['|eyeerr_tmu|'] = np.abs(child_run.data.metrics['eyeerr_tmu'])\n",
    "        child_run.data.metrics['-reward_total'] = - child_run.data.metrics['reward_total']\n",
    "        \n",
    "        for metric in rank_metrics:\n",
    "            if metric not in best_metric or child_run.data.metrics[metric] < best_metric[metric]:\n",
    "                best_metric[metric] = child_run.data.metrics[metric]\n",
    "                best_run[metric] = child_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disturbed-approach",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_values = []\n",
    "for whats_best, run in best_run.items():\n",
    "    best_values = [run.data.metrics[metric] for metric in print_metrics]\n",
    "    metric_values.append([whats_best] + best_values)\n",
    "pd.DataFrame(metric_values, columns=['whats_best'] + print_metrics)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fuzzy-guess",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
