{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pending-partnership",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "from IPython import display\n",
    "from io import BytesIO\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "from stable_baselines.common.cmd_util import make_vec_env\n",
    "from stable_baselines.common.vec_env import VecNormalize\n",
    "\n",
    "import mlflow\n",
    "#import tempfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "settled-freedom",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure logging\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "fhandler = logging.FileHandler(filename='eye-on-stick.log', mode='w')\n",
    "formatter = logging.Formatter('%(asctime)s %(levelname)s - %(message)s')\n",
    "fhandler.setFormatter(formatter)\n",
    "logger.addHandler(fhandler)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "logging.getLogger('gym').setLevel(logging.DEBUG)\n",
    "\n",
    "# suppress trash from PIL and TF\n",
    "# https://github.com/camptocamp/pytest-odoo/issues/15\n",
    "logging.getLogger('PIL').setLevel(logging.ERROR)\n",
    "\n",
    "# https://github.com/hill-a/stable-baselines/issues/298\n",
    "import os\n",
    "# https://stackoverflow.com/questions/40426502/is-there-a-way-to-suppress-the-messages-tensorflow-prints/40426709\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}\n",
    "import warnings\n",
    "# https://stackoverflow.com/questions/15777951/how-to-suppress-pandas-future-warning\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('INFO')\n",
    "tf.autograph.set_verbosity(0)\n",
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liable-semiconductor",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_LOW, X_HIGH = 2, 3\n",
    "Y_LOW, Y_HIGH = -2, 2\n",
    "VX_LOW, VX_HIGH = -0.01, 0.01\n",
    "VY_LOW, VY_HIGH = -0.05, 0.05\n",
    "\n",
    "SCREEN_SIZE = 500\n",
    "SCREEN_SCALE = SCREEN_SIZE / 7\n",
    "BG_COLOR = (0, 0, 0)\n",
    "BORDER_COLOR = (0, 128, 0)\n",
    "\n",
    "TEXT_COLOR = (0, 128, 0)\n",
    "LINE_HEIGHT = 15\n",
    "\n",
    "CIRCLE_SIZE = 0.05\n",
    "TARGET_CIRCLE_COLOR = (255, 0, 0)        \n",
    "EYE_CIRCLE_COLOR = (0, 0, 255)\n",
    "BASE_CIRCLE_COLOR = (0, 0, 255)\n",
    "\n",
    "STICK_LEN = 1.0\n",
    "STICK_WIDTH = 0.01\n",
    "STICK_COLOR = (0, 0, 255)\n",
    "\n",
    "PHI_AMP = np.pi/2\n",
    "DPHI_AMP = 10\n",
    "DPHI = np.pi/360\n",
    "\n",
    "#N_GOALS = 20\n",
    "REWARD_AIM_CLIP = 3\n",
    "N_STEPS = 100\n",
    "\n",
    "N_ENVS = 32\n",
    "N_LEARN_EPOCHS = 2000 * N_ENVS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "national-joshua",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines import PPO2, ACKTR\n",
    "#policy=ACKTR\n",
    "\n",
    "policy_class = PPO2\n",
    "model_name = 'MlpLnLstmPolicy'\n",
    "\n",
    "MODEL_FNAME = None # \"eye-on-stick\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "realistic-deployment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def showarray(img_array):\n",
    "    buf = BytesIO()\n",
    "    Image.fromarray(np.uint8(img_array)).save(buf, 'png')\n",
    "    display.display(display.Image(data=buf.getvalue()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structured-falls",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EyeOnStickEnv(gym.Env):    \n",
    "    metadata = {'render.modes': ['rgb_array']}\n",
    "    \n",
    "    ACC_PLUS = 2\n",
    "    ACC_ZERO = 1\n",
    "    ACC_MINUS = 0\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(EyeOnStickEnv, self).__init__()\n",
    "        self.base_x = 0\n",
    "        self.base_y = 0\n",
    "        self.stick_len = 1.0\n",
    "\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "        self.observation_space = spaces.Box(low=-1, high=1, shape=(4,), dtype=np.float32)\n",
    "        \n",
    "        self.nresets = 0\n",
    "        self.nsteps = 0\n",
    "        \n",
    "        self.reset(reset_pose=True)\n",
    "    \n",
    "    def reset_pose(self):\n",
    "        # the stick is randomly oriented, but stationary\n",
    "        self.phi = 0 # np.random.uniform(low=-np.pi/2, high=np.pi/2)\n",
    "        self.phi_min = self.phi - PHI_AMP\n",
    "        self.phi_max = self.phi + PHI_AMP\n",
    "        self.dphi = 0\n",
    "        \n",
    "        self._recalc()\n",
    "    \n",
    "    def reset(self, reset_pose=False):\n",
    "        self.nresets += 1\n",
    "        self.nsteps = 0\n",
    "        self.actions_log = \"\"\n",
    "        self.info = dict()\n",
    "        self.n_goals = 0\n",
    "\n",
    "        # set random target location\n",
    "        self.target_x = np.random.uniform(low=X_LOW, high=X_HIGH)\n",
    "        self.target_y = np.random.uniform(low=Y_LOW, high=Y_HIGH)\n",
    "        self.target_vx = np.random.uniform(low=VX_LOW, high=VX_HIGH)\n",
    "        self.target_vy = np.random.uniform(low=VY_LOW, high=VY_HIGH)\n",
    "        self.phi_k = 1 # np.random.uniform(low=0.75, high=1.25) * np.random.choice([-1, 1])\n",
    "        \n",
    "        if reset_pose:\n",
    "            self.reset_pose()\n",
    "            # _recalc() is done by reset_pose()\n",
    "        else:\n",
    "            self._recalc()\n",
    "        \n",
    "        return self.get_obs()\n",
    "    \n",
    "    def _recalc(self):    \n",
    "        # eye observes target as projection on retina\n",
    "        self.eye_x = self.stick_len * np.cos(self.phi)\n",
    "        self.eye_y = self.stick_len * np.sin(self.phi)\n",
    "        self.eye_phi = self.phi\n",
    "        \n",
    "        dx = self.target_x - self.eye_x\n",
    "        dy = self.target_y - self.eye_y\n",
    "        self.alpha = np.arctan2(dy, dx) - self.eye_phi\n",
    "              \n",
    "    def get_obs(self):\n",
    "        return np.array([np.sin(self.alpha), np.cos(self.alpha), self.alpha / PHI_AMP, self.dphi / DPHI_AMP]).astype(np.float32)\n",
    "    \n",
    "    def step(self, action):\n",
    "        self.nsteps += 1\n",
    "        \n",
    "        self.target_x += self.target_vx\n",
    "        self.target_y += self.target_vy\n",
    "        \n",
    "        alpha_before = self.alpha\n",
    "\n",
    "        if action == self.ACC_PLUS:\n",
    "            self.dphi += 1\n",
    "            action_char = '+'\n",
    "        elif action == self.ACC_MINUS:\n",
    "            self.dphi -= 1\n",
    "            action_char = '-'\n",
    "        elif action == self.ACC_ZERO:\n",
    "            action_char = 'o'\n",
    "        else:\n",
    "            raise ValueError(\"Received invalid action={} which is not part of the action space\".format(action))\n",
    "\n",
    "        self.actions_log += action_char\n",
    "        if len(self.actions_log) % 75 == 0:\n",
    "            self.actions_log += '\\n'\n",
    "\n",
    "        if self.dphi > DPHI_AMP:\n",
    "            self.dphi = DPHI_AMP\n",
    "        elif self.phi < -DPHI_AMP:\n",
    "            self.dphi = -DPHI_AMP\n",
    "            \n",
    "        self.phi += self.dphi * DPHI * self.phi_k\n",
    "        if self.phi > self.phi_max:\n",
    "            self.phi = self.phi_max\n",
    "            self.dphi = 0\n",
    "        elif self.phi < self.phi_min:\n",
    "            self.phi = self.phi_min\n",
    "            self.dphi = 0\n",
    "            \n",
    "        self._recalc()\n",
    "                \n",
    "        reward_aim = - np.log(np.abs(self.alpha)) - 1 # goes below zero somewhere between 10 and 30 degrees\n",
    "        if reward_aim > REWARD_AIM_CLIP: # 3 - clip to below one degree is perfect)\n",
    "            reward_aim = REWARD_AIM_CLIP\n",
    "        \n",
    "        if reward_aim > 2: # around 3 degrees\n",
    "            self.n_goals += 1\n",
    "        else:\n",
    "            self.n_goals = 0\n",
    "        \n",
    "        reward = reward_aim\n",
    "        done = False # bool(self.n_goals > N_GOALS)\n",
    "            \n",
    "        self.info = dict(n_goals=self.n_goals, reward_aim=f\"{reward_aim:.2f}\", alpha=self.alpha)\n",
    "        return self.get_obs(), reward, done, self.info\n",
    "\n",
    "\n",
    "    def render(self, mode='rgb_array'):\n",
    "        if mode != 'rgb_array':\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        image = Image.new('RGB', (SCREEN_SIZE, SCREEN_SIZE), BG_COLOR)\n",
    "        draw = ImageDraw.Draw(image)\n",
    "        draw.polygon([\n",
    "            (0, 0),\n",
    "            (0, SCREEN_SIZE-1),\n",
    "            (SCREEN_SIZE-1, SCREEN_SIZE-1),\n",
    "            (SCREEN_SIZE-1, 0),\n",
    "            (0, 0)\n",
    "        ], outline=BORDER_COLOR)\n",
    "            \n",
    "        def draw_circle(x, y, r, fill):\n",
    "            px = int(SCREEN_SIZE / 2 + x * SCREEN_SCALE)\n",
    "            py = int(SCREEN_SIZE / 2 + y * SCREEN_SCALE)\n",
    "            pr = int(r * SCREEN_SCALE)\n",
    "            draw.ellipse((px - pr, py - pr, px + pr, py + pr), fill=fill)        \n",
    "\n",
    "        def draw_line(x1, y1, x2, y2, fill, w):\n",
    "            px1 = int(SCREEN_SIZE / 2 + x1 * SCREEN_SCALE)\n",
    "            py1 = int(SCREEN_SIZE / 2 + y1 * SCREEN_SCALE)\n",
    "            px2 = int(SCREEN_SIZE / 2 + x2 * SCREEN_SCALE)\n",
    "            py2 = int(SCREEN_SIZE / 2 + y2 * SCREEN_SCALE)\n",
    "            pw = int(w * SCREEN_SCALE)\n",
    "            draw.line((px1, py1, px2, py2), fill=fill, width=pw)\n",
    "\n",
    "        def draw_text(pos, txt):\n",
    "            draw.text(pos, txt, fill=TEXT_COLOR)\n",
    "            \n",
    "        draw_text((10, LINE_HEIGHT), \"round %d, step %d\" % (self.nresets, self.nsteps))\n",
    "        draw_text((10, 2*LINE_HEIGHT), \"phi_k %.3f, phi %.3f, dphi %.3f, alpha %.3f\" %\n",
    "                  (self.phi_k, self.phi, self.dphi, self.alpha))\n",
    "        draw_text((10, 3*LINE_HEIGHT), \"info %s\" % (str(self.info)))\n",
    "        draw_text((10, 4*LINE_HEIGHT), self.actions_log)\n",
    "            \n",
    "        dx = self.stick_len * np.cos(self.eye_phi)\n",
    "        dy = self.stick_len * np.sin(self.eye_phi)\n",
    "        draw_circle(self.base_x, self.base_y, CIRCLE_SIZE, BASE_CIRCLE_COLOR)\n",
    "        draw_line(self.base_x, self.base_y, self.base_x + dx, self.base_y + dy, STICK_COLOR, STICK_WIDTH)\n",
    "        draw_circle(self.eye_x, self.eye_y, CIRCLE_SIZE, EYE_CIRCLE_COLOR)\n",
    "        draw_circle(self.target_x, self.target_y, CIRCLE_SIZE, TARGET_CIRCLE_COLOR)\n",
    "               \n",
    "        return np.asarray(image)\n",
    "\n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dying-copying",
   "metadata": {},
   "outputs": [],
   "source": [
    "#eos = EyeOnStickEnv()\n",
    "#eos.reset()\n",
    "#eos.get_obs()\n",
    "#eos.step(1)\n",
    "#plt.imshow(eos.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mexican-burst",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_vec_env(lambda: EyeOnStickEnv(), n_envs=N_ENVS)\n",
    "env = VecNormalize(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "macro-chamber",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = None\n",
    "#if MODEL_FNAME is not None:\n",
    "#    try:\n",
    "#        model = PPO2.load(MODEL_FNAME)\n",
    "#        logger.debug(\"model loaded from '%s'\" % (MODEL_FNAME))\n",
    "#    except ValueError as ex:\n",
    "#        logger.debug(\"could not load model from '%s', starting anew\" % (MODEL_FNAME))\n",
    "#else:\n",
    "#    logger.debug(\"model loading disabled, starting anew\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gothic-persian",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "\n",
    "import git\n",
    "repo = git.Repo()\n",
    "assert not repo.is_dirty()\n",
    "\n",
    "with mlflow.start_run(run_name='eye-on-stick neglog nogoals') as parent_run:\n",
    "    mlflow_artifacts_dir = urllib.request.url2pathname(urllib.parse.urlparse(mlflow.get_artifact_uri()).path)\n",
    "    \n",
    "    mlflow.log_param(\"policy_class\", policy_class.__name__)\n",
    "    mlflow.log_param(\"model_name\", model_name)\n",
    "    \n",
    "    tensorboard_logdir = os.path.join(mlflow_artifacts_dir, \"tensorboard_log\")\n",
    "    os.makedirs(tensorboard_logdir, exist_ok=False)\n",
    "    \n",
    "    model = policy_class(model_name, env, verbose=1, tensorboard_log=tensorboard_logdir)\n",
    "    \n",
    "    for era in range(1000000000): # FIXME\n",
    "        with mlflow.start_run(run_name=f'era={era}', nested=True) as child_run:\n",
    "            total_reward = 0\n",
    "            alphas = []\n",
    "\n",
    "            obs = env.env_method('reset', reset_pose=True)\n",
    "\n",
    "            for _ in range(N_STEPS):\n",
    "                display.clear_output(wait=True)\n",
    "                showarray(env.render(mode='rgb_array'))\n",
    "\n",
    "                actions, _ = model.predict(obs, deterministic=True)\n",
    "                obs, rewards, _dones, infos = env.step(actions)\n",
    "                \n",
    "                total_reward += np.sum(rewards)\n",
    "                alphas.append([info['alpha'] for info in infos])                \n",
    "            \n",
    "            mlflow.log_metric(key=\"alpha_mean\", value=np.mean(alphas), step=era)\n",
    "            mlflow.log_metric(key=\"alpha_std\", value=np.std(alphas), step=era)\n",
    "            mlflow.log_metric(key=\"total_reward\", value=total_reward, step=era)\n",
    "\n",
    "            model.learn(N_LEARN_EPOCHS)\n",
    "            logger.debug(\"learning completed\")\n",
    "\n",
    "            #mlflow.keras.save_model(model, f\"era{era}\")\n",
    " \n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stupid-avatar",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
